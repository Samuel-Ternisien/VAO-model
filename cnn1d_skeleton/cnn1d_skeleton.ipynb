{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/opt/anaconda3/envs/fisa_env/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fisa_env/lib/python3.10/site-packages/torch/__init__.py:367\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    366\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSymInt\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: /opt/anaconda3/envs/fisa_env/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: cuda\n",
      "GPU: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_read_csv(path):\n",
    "    for sep in [\";\", \"\\t\", \",\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=sep)\n",
    "            if df.shape[1] > 1:\n",
    "                return df\n",
    "        except:\n",
    "            pass\n",
    "    raise ValueError(f\"Impossible de lire {path}\")\n",
    "def find_time_col(df):\n",
    "    for c in df.columns:\n",
    "        if \"time\" in c.lower():\n",
    "            return c\n",
    "    raise ValueError(\"Colonne temps non trouvée\")\n",
    "def read_and_slice_by_time(path, t0, t1):\n",
    "    df = smart_read_csv(path)\n",
    "    tcol = find_time_col(df)\n",
    "\n",
    "    df = df[(df[tcol] >= t0) & (df[tcol] <= t1)]\n",
    "    df = df.drop(columns=[tcol])\n",
    "\n",
    "    return torch.tensor(df.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_L(x, L):\n",
    "    T, C = x.shape\n",
    "\n",
    "    if T == 0:\n",
    "        return torch.zeros(L, C)\n",
    "\n",
    "    if T == L:\n",
    "        return x\n",
    "\n",
    "    idx = torch.linspace(0, T - 1, L)\n",
    "    idx0 = idx.long()\n",
    "    idx1 = torch.clamp(idx0 + 1, max=T - 1)\n",
    "    w = idx - idx0\n",
    "\n",
    "    return (1 - w).unsqueeze(1) * x[idx0] + w.unsqueeze(1) * x[idx1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOTS = {\n",
    "    \"plantar\": \"/home/fisa/stockage1/mindscan/Plantar_activity\",\n",
    "    \"events\":  \"/home/fisa/stockage1/mindscan/Skeleton\"\n",
    "}\n",
    "\n",
    "FILENAMES = {\n",
    "    \"skeleton\": \"skeleton.csv\"\n",
    "}\n",
    "\n",
    "L = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segments: 10204\n"
     ]
    }
   ],
   "source": [
    "def build_segments_index(events_root):\n",
    "    rows = []\n",
    "\n",
    "    subjects = sorted([\n",
    "        d for d in os.listdir(events_root)\n",
    "        if os.path.isdir(os.path.join(events_root, d))\n",
    "    ])\n",
    "\n",
    "    for subject in subjects:\n",
    "        for seq in os.listdir(os.path.join(events_root, subject)):\n",
    "            classif = os.path.join(events_root, subject, seq, \"classif.csv\")\n",
    "            if not os.path.exists(classif):\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(classif, sep=\";\")\n",
    "            for _, r in df.iterrows():\n",
    "                rows.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"seq\": seq,\n",
    "                    \"label\": int(r[\"Class\"]),\n",
    "                    \"t0\": float(r[\"Timestamp Start\"]),\n",
    "                    \"t1\": float(r[\"Timestamp End\"]),\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "segments = build_segments_index(ROOTS[\"events\"])\n",
    "print(\"Total segments:\", len(segments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train segments: 6084\n",
      "Val segments: 1569\n"
     ]
    }
   ],
   "source": [
    "allowed_subjects = {f\"S{str(i).zfill(2)}\" for i in range(1, 25)}\n",
    "segments = segments[segments[\"subject\"].isin(allowed_subjects)].reset_index(drop=True)\n",
    "\n",
    "subjects = sorted(segments[\"subject\"].unique())\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(subjects)\n",
    "\n",
    "n_train = int(0.8 * len(subjects))\n",
    "train_subjects = set(subjects[:n_train])\n",
    "val_subjects   = set(subjects[n_train:])\n",
    "\n",
    "train_segments = segments[segments[\"subject\"].isin(train_subjects)].reset_index(drop=True)\n",
    "val_segments   = segments[segments[\"subject\"].isin(val_subjects)].reset_index(drop=True)\n",
    "\n",
    "print(\"Train segments:\", len(train_segments))\n",
    "print(\"Val segments:\", len(val_segments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkeletonEventDataset(Dataset):\n",
    "    def __init__(self, segments_df, roots, filenames, L=256):\n",
    "        self.df = segments_df.reset_index(drop=True)\n",
    "        self.roots = roots\n",
    "        self.filenames = filenames\n",
    "        self.L = L\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        subject, seq = r[\"subject\"], r[\"seq\"]\n",
    "        t0, t1 = float(r[\"t0\"]), float(r[\"t1\"])\n",
    "        y = int(r[\"label\"]) - 1\n",
    "\n",
    "        path = os.path.join(\n",
    "            self.roots[\"skeleton\"], subject, seq, self.filenames[\"skeleton\"]\n",
    "        )\n",
    "\n",
    "        x = read_and_slice_by_time(path, t0, t1)   # [T, C]\n",
    "        x = resample_to_L(x, self.L)               # [256, C]\n",
    "        x = x.transpose(0, 1).contiguous()         # [C, 256]\n",
    "\n",
    "        return {\n",
    "            \"skeleton\": x,\n",
    "            \"y\": torch.tensor(y, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch: torch.Size([128, 50, 256]) torch.Size([128])\n",
      "Val batch: torch.Size([128, 50, 256]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "train_ds = SkeletonEventDataset(train_segments, ROOTS, FILENAMES, L)\n",
    "val_ds   = SkeletonEventDataset(val_segments, ROOTS, FILENAMES, L)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "b = next(iter(train_dl))\n",
    "print(\"Train batch:\", b[\"skeleton\"].shape, b[\"y\"].shape)\n",
    "\n",
    "b = next(iter(val_dl))\n",
    "print(\"Val batch:\", b[\"skeleton\"].shape, b[\"y\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skeleton_CNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=31):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 64, 7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, 5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(dim=-1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 48/48 [00:42<00:00,  1.14it/s, acc=0.271, loss=2.8478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 48/48 [00:37<00:00,  1.26it/s, acc=0.536, loss=1.5738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 48/48 [00:38<00:00,  1.26it/s, acc=0.613, loss=1.3058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 48/48 [00:39<00:00,  1.22it/s, acc=0.652, loss=1.1438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 48/48 [00:37<00:00,  1.28it/s, acc=0.681, loss=1.0217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 48/48 [00:37<00:00,  1.29it/s, acc=0.704, loss=0.9534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 48/48 [00:37<00:00,  1.29it/s, acc=0.711, loss=0.9006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 48/48 [00:37<00:00,  1.28it/s, acc=0.726, loss=0.8701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 48/48 [00:37<00:00,  1.29it/s, acc=0.731, loss=0.8368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 48/48 [00:38<00:00,  1.26it/s, acc=0.761, loss=0.7577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 48/48 [00:39<00:00,  1.22it/s, acc=0.755, loss=0.7518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 48/48 [00:36<00:00,  1.31it/s, acc=0.775, loss=0.6919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 48/48 [00:38<00:00,  1.25it/s, acc=0.784, loss=0.6624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 48/48 [00:39<00:00,  1.23it/s, acc=0.791, loss=0.6384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 48/48 [00:37<00:00,  1.27it/s, acc=0.808, loss=0.5887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 48/48 [00:37<00:00,  1.27it/s, acc=0.815, loss=0.5741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 48/48 [00:37<00:00,  1.27it/s, acc=0.821, loss=0.5590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 48/48 [00:38<00:00,  1.26it/s, acc=0.819, loss=0.5678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 48/48 [00:37<00:00,  1.29it/s, acc=0.834, loss=0.5102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 48/48 [00:37<00:00,  1.27it/s, acc=0.834, loss=0.5070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.717\n"
     ]
    }
   ],
   "source": [
    "def train_skeleton(model, train_dl, val_dl, epochs=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct = total = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            x = batch[\"skeleton\"].to(device, non_blocking=True)\n",
    "            y = batch[\"y\"].to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "            total += x.size(0)\n",
    "\n",
    "            pbar.set_postfix(\n",
    "                loss=f\"{running_loss/total:.4f}\",\n",
    "                acc=f\"{correct/total:.3f}\"\n",
    "            )\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                x = batch[\"skeleton\"].to(device)\n",
    "                y = batch[\"y\"].to(device)\n",
    "                correct += (model(x).argmax(1) == y).sum().item()\n",
    "                total += x.size(0)\n",
    "\n",
    "        print(f\"Val acc: {correct/total:.3f}\")\n",
    "        \n",
    "in_channels = b[\"skeleton\"].shape[1]\n",
    "\n",
    "model = Skeleton_CNN(in_channels).to(device)\n",
    "train_skeleton(model, train_dl, val_dl, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy (Plantar): 0.717\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x = batch[\"skeleton\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return correct / total\n",
    "val_acc = evaluate(model, val_dl)\n",
    "print(f\"Validation accuracy (Skeleton): {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vrai label : 22\n",
      "Label prédit : 24\n"
     ]
    }
   ],
   "source": [
    "sample = val_ds[3]\n",
    "\n",
    "x = sample[\"skeleton\"].unsqueeze(0).to(device)  # [1, C, 256]\n",
    "y_true = sample[\"y\"].item()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "    y_pred = logits.argmax(dim=1).item()\n",
    "\n",
    "print(\"Vrai label :\", y_true)\n",
    "print(\"Label prédit :\", y_pred)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaze_skill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
