{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: cuda\n",
      "GPU: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_read_csv(path):\n",
    "    for sep in [\";\", \"\\t\", \",\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=sep)\n",
    "            if df.shape[1] > 1:\n",
    "                return df\n",
    "        except:\n",
    "            pass\n",
    "    raise ValueError(f\"Impossible de lire {path}\")\n",
    "def find_time_col(df):\n",
    "    for c in df.columns:\n",
    "        if \"time\" in c.lower():\n",
    "            return c\n",
    "    raise ValueError(\"Colonne temps non trouvée\")\n",
    "def read_and_slice_by_time(path, t0, t1):\n",
    "    df = smart_read_csv(path)\n",
    "    tcol = find_time_col(df)\n",
    "\n",
    "    df = df[(df[tcol] >= t0) & (df[tcol] <= t1)]\n",
    "    df = df.drop(columns=[tcol])\n",
    "\n",
    "    return torch.tensor(df.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_L(x, L):\n",
    "    T, C = x.shape\n",
    "\n",
    "    if T == 0:\n",
    "        return torch.zeros(L, C)\n",
    "\n",
    "    if T == L:\n",
    "        return x\n",
    "\n",
    "    idx = torch.linspace(0, T - 1, L)\n",
    "    idx0 = idx.long()\n",
    "    idx1 = torch.clamp(idx0 + 1, max=T - 1)\n",
    "    w = idx - idx0\n",
    "\n",
    "    return (1 - w).unsqueeze(1) * x[idx0] + w.unsqueeze(1) * x[idx1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOTS = {\n",
    "    \"plantar\": \"/home/fisa/stockage1/mindscan/Plantar_activity\",\n",
    "    \"events\":  \"/home/fisa/stockage1/mindscan/Events\"\n",
    "}\n",
    "\n",
    "FILENAMES = {\n",
    "    \"plantar\": \"insoles.csv\"\n",
    "}\n",
    "\n",
    "L = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segments: 10204\n"
     ]
    }
   ],
   "source": [
    "def build_segments_index(events_root):\n",
    "    rows = []\n",
    "\n",
    "    subjects = sorted([\n",
    "        d for d in os.listdir(events_root)\n",
    "        if os.path.isdir(os.path.join(events_root, d))\n",
    "    ])\n",
    "\n",
    "    for subject in subjects:\n",
    "        for seq in os.listdir(os.path.join(events_root, subject)):\n",
    "            classif = os.path.join(events_root, subject, seq, \"classif.csv\")\n",
    "            if not os.path.exists(classif):\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(classif, sep=\";\")\n",
    "            for _, r in df.iterrows():\n",
    "                rows.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"seq\": seq,\n",
    "                    \"label\": int(r[\"Class\"]),\n",
    "                    \"t0\": float(r[\"Timestamp Start\"]),\n",
    "                    \"t1\": float(r[\"Timestamp End\"]),\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "segments = build_segments_index(ROOTS[\"events\"])\n",
    "print(\"Total segments:\", len(segments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train segments: 6084\n",
      "Val segments: 1569\n"
     ]
    }
   ],
   "source": [
    "allowed_subjects = {f\"S{str(i).zfill(2)}\" for i in range(1, 25)}\n",
    "segments = segments[segments[\"subject\"].isin(allowed_subjects)].reset_index(drop=True)\n",
    "\n",
    "subjects = sorted(segments[\"subject\"].unique())\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(subjects)\n",
    "\n",
    "n_train = int(0.8 * len(subjects))\n",
    "train_subjects = set(subjects[:n_train])\n",
    "val_subjects   = set(subjects[n_train:])\n",
    "\n",
    "train_segments = segments[segments[\"subject\"].isin(train_subjects)].reset_index(drop=True)\n",
    "val_segments   = segments[segments[\"subject\"].isin(val_subjects)].reset_index(drop=True)\n",
    "\n",
    "print(\"Train segments:\", len(train_segments))\n",
    "print(\"Val segments:\", len(val_segments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantarEventDataset(Dataset):\n",
    "    def __init__(self, segments_df, roots, filenames, L=256):\n",
    "        self.df = segments_df.reset_index(drop=True)\n",
    "        self.roots = roots\n",
    "        self.filenames = filenames\n",
    "        self.L = L\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        subject, seq = r[\"subject\"], r[\"seq\"]\n",
    "        t0, t1 = float(r[\"t0\"]), float(r[\"t1\"])\n",
    "        y = int(r[\"label\"]) - 1\n",
    "\n",
    "        path = os.path.join(\n",
    "            self.roots[\"plantar\"], subject, seq, self.filenames[\"plantar\"]\n",
    "        )\n",
    "\n",
    "        x = read_and_slice_by_time(path, t0, t1)   # [T, C]\n",
    "        x = resample_to_L(x, self.L)               # [256, C]\n",
    "        x = x.transpose(0, 1).contiguous()         # [C, 256]\n",
    "\n",
    "        return {\n",
    "            \"plantar\": x,\n",
    "            \"y\": torch.tensor(y, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch: torch.Size([128, 50, 256]) torch.Size([128])\n",
      "Val batch: torch.Size([128, 50, 256]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "train_ds = PlantarEventDataset(train_segments, ROOTS, FILENAMES, L)\n",
    "val_ds   = PlantarEventDataset(val_segments, ROOTS, FILENAMES, L)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "b = next(iter(train_dl))\n",
    "print(\"Train batch:\", b[\"plantar\"].shape, b[\"y\"].shape)\n",
    "\n",
    "b = next(iter(val_dl))\n",
    "print(\"Val batch:\", b[\"plantar\"].shape, b[\"y\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plantar_CNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=31):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 64, 7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, 5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(dim=-1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 48/48 [00:43<00:00,  1.11it/s, acc=0.263, loss=3.0951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 48/48 [00:40<00:00,  1.18it/s, acc=0.518, loss=1.6269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 48/48 [00:40<00:00,  1.17it/s, acc=0.610, loss=1.3068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 48/48 [00:41<00:00,  1.15it/s, acc=0.660, loss=1.1370]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 48/48 [00:40<00:00,  1.18it/s, acc=0.677, loss=1.0441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 48/48 [00:44<00:00,  1.07it/s, acc=0.708, loss=0.9434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 48/48 [00:44<00:00,  1.07it/s, acc=0.723, loss=0.8905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 48/48 [00:41<00:00,  1.15it/s, acc=0.707, loss=0.9333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 48/48 [00:42<00:00,  1.13it/s, acc=0.723, loss=0.8751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 48/48 [00:39<00:00,  1.22it/s, acc=0.743, loss=0.7978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.704\n"
     ]
    }
   ],
   "source": [
    "def train_plantar(model, train_dl, val_dl, epochs=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct = total = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            x = batch[\"plantar\"].to(device, non_blocking=True)\n",
    "            y = batch[\"y\"].to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "            total += x.size(0)\n",
    "\n",
    "            pbar.set_postfix(\n",
    "                loss=f\"{running_loss/total:.4f}\",\n",
    "                acc=f\"{correct/total:.3f}\"\n",
    "            )\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                x = batch[\"plantar\"].to(device)\n",
    "                y = batch[\"y\"].to(device)\n",
    "                correct += (model(x).argmax(1) == y).sum().item()\n",
    "                total += x.size(0)\n",
    "\n",
    "        print(f\"Val acc: {correct/total:.3f}\")\n",
    "        \n",
    "in_channels = b[\"plantar\"].shape[1]\n",
    "\n",
    "model = Plantar_CNN(in_channels).to(device)\n",
    "train_plantar(model, train_dl, val_dl, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy (Plantar): 0.666\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x = batch[\"plantar\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return correct / total\n",
    "val_acc = evaluate(model, val_dl)\n",
    "print(f\"Validation accuracy (Plantar): {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vrai label : 4\n",
      "Label prédit : 29\n"
     ]
    }
   ],
   "source": [
    "sample = val_ds[100]\n",
    "\n",
    "x = sample[\"plantar\"].unsqueeze(0).to(device)  # [1, C, 256]\n",
    "y_true = sample[\"y\"].item()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "    y_pred = logits.argmax(dim=1).item()\n",
    "\n",
    "print(\"Vrai label :\", y_true)\n",
    "print(\"Label prédit :\", y_pred)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
