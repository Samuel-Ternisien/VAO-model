{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 1D multimodal (IMU + Plantar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: cuda\n",
      "GPU: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_read_csv(path):\n",
    "    for sep in [\";\", \"\\t\", \",\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=sep)\n",
    "            if df.shape[1] > 1:\n",
    "                return df\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise ValueError(f\"Impossible de lire {path}\")\n",
    "\n",
    "\n",
    "def find_time_col(df):\n",
    "    for c in df.columns:\n",
    "        if \"time\" in c.lower():\n",
    "            return c\n",
    "    raise ValueError(\"Colonne temps non trouvee\")\n",
    "\n",
    "\n",
    "def read_and_slice_by_time(path, t0, t1):\n",
    "    df = smart_read_csv(path)\n",
    "    tcol = find_time_col(df)\n",
    "\n",
    "    df = df[(df[tcol] >= t0) & (df[tcol] <= t1)]\n",
    "    df = df.drop(columns=[tcol])\n",
    "\n",
    "    return torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def resample_to_L(x, L):\n",
    "    \"\"\"\n",
    "    x : [T, C] -> [L, C]\n",
    "    \"\"\"\n",
    "    T, C = x.shape\n",
    "\n",
    "    if T == 0:\n",
    "        return torch.zeros(L, C)\n",
    "\n",
    "    if T == L:\n",
    "        return x\n",
    "\n",
    "    idx = torch.linspace(0, T - 1, L)\n",
    "    idx0 = idx.long()\n",
    "    idx1 = torch.clamp(idx0 + 1, max=T - 1)\n",
    "    w = idx - idx0\n",
    "\n",
    "    return (1 - w).unsqueeze(1) * x[idx0] + w.unsqueeze(1) * x[idx1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOTS = {\n",
    "    \"imu\": \"/home/fisa/stockage1/mindscan/IMU\",\n",
    "    \"plantar\": \"/home/fisa/stockage1/mindscan/Plantar_activity\",\n",
    "    \"events\": \"/home/fisa/stockage1/mindscan/Events\",\n",
    "}\n",
    "\n",
    "FILENAMES = {\n",
    "    \"imu\": \"imu.csv\",\n",
    "    \"plantar\": \"insoles.csv\",\n",
    "}\n",
    "\n",
    "L = 256\n",
    "NUM_CLASSES = 31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segments: 10204\n"
     ]
    }
   ],
   "source": [
    "def build_segments_index(events_root):\n",
    "    rows = []\n",
    "    subjects = sorted([\n",
    "        d for d in os.listdir(events_root)\n",
    "        if os.path.isdir(os.path.join(events_root, d))\n",
    "    ])\n",
    "\n",
    "    for subject in subjects:\n",
    "        for seq in os.listdir(os.path.join(events_root, subject)):\n",
    "            classif = os.path.join(events_root, subject, seq, \"classif.csv\")\n",
    "            if not os.path.exists(classif):\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(classif, sep=\";\")\n",
    "            for _, r in df.iterrows():\n",
    "                rows.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"seq\": seq,\n",
    "                    \"label\": int(r[\"Class\"]),\n",
    "                    \"t0\": float(r[\"Timestamp Start\"]),\n",
    "                    \"t1\": float(r[\"Timestamp End\"]),\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "segments = build_segments_index(ROOTS[\"events\"])\n",
    "print(\"Total segments:\", len(segments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train segments: 6084\n",
      "Val segments: 1569\n"
     ]
    }
   ],
   "source": [
    "allowed_subjects = {f\"S{str(i).zfill(2)}\" for i in range(1, 25)}\n",
    "segments = segments[segments[\"subject\"].isin(allowed_subjects)].reset_index(drop=True)\n",
    "\n",
    "subjects = sorted(segments[\"subject\"].unique())\n",
    "print(\"Total segments after subject filter:\", len(segments))\n",
    "print(\"Total subjects:\", len(subjects), subjects)\n",
    "\n",
    "K_FOLDS = 5\n",
    "EPOCHS = 20\n",
    "PATIENCE = 4\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "if len(subjects) < K_FOLDS:\n",
    "    raise ValueError(f\"Not enough subjects ({len(subjects)}) for {K_FOLDS}-fold CV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalEventDataset(Dataset):\n",
    "    def __init__(self, segments_df, roots, filenames, L=256):\n",
    "        self.df = segments_df.reset_index(drop=True)\n",
    "        self.roots = roots\n",
    "        self.filenames = filenames\n",
    "        self.L = L\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        subject, seq = r[\"subject\"], r[\"seq\"]\n",
    "        t0, t1 = float(r[\"t0\"]), float(r[\"t1\"])\n",
    "        y = int(r[\"label\"]) - 1\n",
    "\n",
    "        imu_path = os.path.join(self.roots[\"imu\"], subject, seq, self.filenames[\"imu\"])\n",
    "        plantar_path = os.path.join(self.roots[\"plantar\"], subject, seq, self.filenames[\"plantar\"])\n",
    "\n",
    "        imu = read_and_slice_by_time(imu_path, t0, t1)            # [T, 96]\n",
    "        imu = resample_to_L(imu, self.L).transpose(0, 1).contiguous()  # [96, 256]\n",
    "\n",
    "        plantar = read_and_slice_by_time(plantar_path, t0, t1)    # [T, C]\n",
    "        plantar = resample_to_L(plantar, self.L).transpose(0, 1).contiguous()  # [C, 256]\n",
    "\n",
    "        return {\n",
    "            \"imu\": imu,\n",
    "            \"plantar\": plantar,\n",
    "            \"y\": torch.tensor(y, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch IMU: torch.Size([128, 96, 256])\n",
      "Train batch Plantar: torch.Size([128, 50, 256])\n",
      "Train batch y: torch.Size([128])\n",
      "Channels -> IMU: 96 | Plantar: 50\n"
     ]
    }
   ],
   "source": [
    "def make_dataloaders(train_segments, val_segments, batch_size=128, num_workers=8):\n",
    "    train_ds = MultiModalEventDataset(train_segments, ROOTS, FILENAMES, L)\n",
    "    val_ds = MultiModalEventDataset(val_segments, ROOTS, FILENAMES, L)\n",
    "\n",
    "    train_dl = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=(num_workers > 0),\n",
    "    )\n",
    "\n",
    "    val_dl = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=(num_workers > 0),\n",
    "    )\n",
    "\n",
    "    sample = train_ds[0]\n",
    "    imu_channels = sample[\"imu\"].shape[0]\n",
    "    plantar_channels = sample[\"plantar\"].shape[0]\n",
    "\n",
    "    return train_dl, val_dl, imu_channels, plantar_channels, train_ds, val_ds\n",
    "\n",
    "\n",
    "def compute_class_weights(train_segments, num_classes, device):\n",
    "    y = train_segments[\"label\"].astype(int).values - 1\n",
    "    counts = np.bincount(y, minlength=num_classes).astype(np.float32)\n",
    "    inv_freq = len(y) / (num_classes * np.maximum(counts, 1.0))\n",
    "    weights = torch.tensor(inv_freq, dtype=torch.float32, device=device)\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBranch(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 64, 7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, 5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x.mean(dim=-1)  # [B, 256]\n",
    "\n",
    "\n",
    "class IMUPlantarMultiCNN(nn.Module):\n",
    "    def __init__(self, imu_in_channels=96, plantar_in_channels=32, num_classes=31):\n",
    "        super().__init__()\n",
    "        self.imu_branch = ConvBranch(imu_in_channels)\n",
    "        self.plantar_branch = ConvBranch(plantar_in_channels)\n",
    "\n",
    "        # Fusion multilayer des deux representations [256 + 256]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, imu_x, plantar_x):\n",
    "        imu_feat = self.imu_branch(imu_x)\n",
    "        plantar_feat = self.plantar_branch(plantar_x)\n",
    "        fused = torch.cat([imu_feat, plantar_feat], dim=1)\n",
    "        return self.classifier(fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            imu_x = batch[\"imu\"].to(device)\n",
    "            plantar_x = batch[\"plantar\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "\n",
    "            logits = model(imu_x, plantar_x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    acc = correct / total\n",
    "    f1 = f1_score(all_targets, all_preds, average=\"macro\", zero_division=0)\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "def train_multimodal(\n",
    "    model,\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    class_weights=None,\n",
    "    epochs=20,\n",
    "    lr=1e-3,\n",
    "    patience=4,\n",
    "    min_delta=1e-4,\n",
    "):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    best_val_f1 = -1.0\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    best_state = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = total = 0\n",
    "\n",
    "        pbar = tqdm(train_dl, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            imu_x = batch[\"imu\"].to(device, non_blocking=True)\n",
    "            plantar_x = batch[\"plantar\"].to(device, non_blocking=True)\n",
    "            y = batch[\"y\"].to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(imu_x, plantar_x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * imu_x.size(0)\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "            total += imu_x.size(0)\n",
    "\n",
    "            pbar.set_postfix(\n",
    "                loss=f\"{running_loss / total:.4f}\",\n",
    "                acc=f\"{correct / total:.3f}\",\n",
    "            )\n",
    "\n",
    "        val_acc, val_f1 = evaluate(model, val_dl)\n",
    "        print(f\"Val acc: {val_acc:.3f} | Val F1-macro: {val_f1:.3f}\")\n",
    "\n",
    "        if val_f1 > best_val_f1 + min_delta:\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} (best epoch: {best_epoch}, best F1: {best_val_f1:.3f})\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return {\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"best_val_f1\": best_val_f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 48/48 [04:23<00:00,  5.48s/it, acc=0.290, loss=2.4598] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  79%|███████▉  | 38/48 [02:46<00:30,  3.01s/it, acc=0.632, loss=1.1416]"
     ]
    }
   ],
   "source": [
    "subjects_np = np.array(subjects)\n",
    "\n",
    "subject_majority_labels = []\n",
    "for s in subjects_np:\n",
    "    y_s = segments[segments[\"subject\"] == s][\"label\"].astype(int) - 1\n",
    "    subject_majority_labels.append(int(y_s.mode().iloc[0]))\n",
    "subject_majority_labels = np.array(subject_majority_labels)\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "last_model, last_val_ds = None, None\n",
    "\n",
    "for fold_id, (train_idx, val_idx) in enumerate(\n",
    "    sgkf.split(subjects_np, subject_majority_labels, groups=subjects_np),\n",
    "    start=1,\n",
    "):\n",
    "    train_subjects = set(subjects_np[train_idx])\n",
    "    val_subjects = set(subjects_np[val_idx])\n",
    "\n",
    "    train_segments = segments[segments[\"subject\"].isin(train_subjects)].reset_index(drop=True)\n",
    "    val_segments = segments[segments[\"subject\"].isin(val_subjects)].reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n===== Fold {fold_id}/{K_FOLDS} =====\")\n",
    "    print(f\"Train subjects ({len(train_subjects)}): {sorted(train_subjects)}\")\n",
    "    print(f\"Val subjects ({len(val_subjects)}): {sorted(val_subjects)}\")\n",
    "    print(f\"Train segments: {len(train_segments)} | Val segments: {len(val_segments)}\")\n",
    "\n",
    "    train_dl, val_dl, imu_channels, plantar_channels, train_ds, val_ds = make_dataloaders(\n",
    "        train_segments, val_segments, batch_size=128, num_workers=8\n",
    "    )\n",
    "\n",
    "    class_weights = compute_class_weights(train_segments, NUM_CLASSES, device)\n",
    "\n",
    "    model = IMUPlantarMultiCNN(\n",
    "        imu_in_channels=imu_channels,\n",
    "        plantar_in_channels=plantar_channels,\n",
    "        num_classes=NUM_CLASSES,\n",
    "    ).to(device)\n",
    "\n",
    "    train_info = train_multimodal(\n",
    "        model,\n",
    "        train_dl,\n",
    "        val_dl,\n",
    "        class_weights=class_weights,\n",
    "        epochs=EPOCHS,\n",
    "        lr=1e-3,\n",
    "        patience=PATIENCE,\n",
    "        min_delta=MIN_DELTA,\n",
    "    )\n",
    "\n",
    "    val_acc, val_f1 = evaluate(model, val_dl)\n",
    "    last_model, last_val_ds = model, val_ds\n",
    "\n",
    "    fold_results.append({\n",
    "        \"fold\": fold_id,\n",
    "        \"best_epoch\": train_info[\"best_epoch\"],\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1_macro\": val_f1,\n",
    "        \"n_train_segments\": len(train_segments),\n",
    "        \"n_val_segments\": len(val_segments),\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold_id} final -> Acc: {val_acc:.3f} | F1-macro: {val_f1:.3f} | Best epoch: {train_info['best_epoch']}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/fisa/pirosa/test_robyn/cnn1d_multimodal_imu_plantar.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.167.254.21/home/fisa/pirosa/test_robyn/cnn1d_multimodal_imu_plantar.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m             total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m y\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.167.254.21/home/fisa/pirosa/test_robyn/cnn1d_multimodal_imu_plantar.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m correct \u001b[39m/\u001b[39m total\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B194.167.254.21/home/fisa/pirosa/test_robyn/cnn1d_multimodal_imu_plantar.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m val_acc \u001b[39m=\u001b[39m evaluate(model, val_dl)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.167.254.21/home/fisa/pirosa/test_robyn/cnn1d_multimodal_imu_plantar.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidation accuracy (IMU + Plantar): \u001b[39m\u001b[39m{\u001b[39;00mval_acc\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(fold_results)\n",
    "cv_results\n",
    "\n",
    "print(f\"\\nCV mean accuracy: {cv_results['val_acc'].mean():.3f} +/- {cv_results['val_acc'].std(ddof=1):.3f}\")\n",
    "print(f\"CV mean F1-macro: {cv_results['val_f1_macro'].mean():.3f} +/- {cv_results['val_f1_macro'].std(ddof=1):.3f}\")\n",
    "print(f\"CV mean best epoch: {cv_results['best_epoch'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = last_val_ds[0]\n",
    "imu_x = sample[\"imu\"].unsqueeze(0).to(device)\n",
    "plantar_x = sample[\"plantar\"].unsqueeze(0).to(device)\n",
    "y_true = sample[\"y\"].item()\n",
    "\n",
    "last_model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = last_model(imu_x, plantar_x)\n",
    "    y_pred = logits.argmax(dim=1).item()\n",
    "\n",
    "print(\"Vrai label :\", y_true)\n",
    "print(\"Label predit :\", y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
