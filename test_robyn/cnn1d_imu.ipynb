{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: cuda\n",
      "GPU: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_read_csv(path):\n",
    "    for sep in [\";\", \"\\t\", \",\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=sep)\n",
    "            if df.shape[1] > 1:\n",
    "                return df\n",
    "        except:\n",
    "            pass\n",
    "    raise ValueError(f\"Impossible de lire {path}\")\n",
    "\n",
    "def find_time_col(df):\n",
    "    for c in df.columns:\n",
    "        if \"time\" in c.lower():\n",
    "            return c\n",
    "    raise ValueError(\"Colonne temps non trouvée\")\n",
    "\n",
    "def read_and_slice_by_time(path, t0, t1):\n",
    "    df = smart_read_csv(path)\n",
    "    tcol = find_time_col(df)\n",
    "\n",
    "    df = df[(df[tcol] >= t0) & (df[tcol] <= t1)]\n",
    "    df = df.drop(columns=[tcol])\n",
    "\n",
    "    x = torch.tensor(df.values, dtype=torch.float32)\n",
    "    return x\n",
    "\n",
    "def resample_to_L(x, L):\n",
    "    \"\"\"\n",
    "    x : [T, C] -> [L, C]\n",
    "    \"\"\"\n",
    "    T, C = x.shape\n",
    "\n",
    "    if T == 0:\n",
    "        return torch.zeros(L, C)\n",
    "\n",
    "    if T == L:\n",
    "        return x\n",
    "\n",
    "    idx = torch.linspace(0, T - 1, L)\n",
    "    idx0 = idx.long()\n",
    "    idx1 = torch.clamp(idx0 + 1, max=T - 1)\n",
    "    w = idx - idx0\n",
    "\n",
    "    return (1 - w).unsqueeze(1) * x[idx0] + w.unsqueeze(1) * x[idx1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOTS = {\n",
    "    \"imu\": \"/home/fisa/stockage1/mindscan/IMU\",\n",
    "    \"events\": \"/home/fisa/stockage1/mindscan/Events\"\n",
    "}\n",
    "\n",
    "FILENAMES = {\n",
    "    \"imu\": \"imu.csv\"\n",
    "}\n",
    "\n",
    "L = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segments: 10204\n"
     ]
    }
   ],
   "source": [
    "def build_segments_index(events_root):\n",
    "    rows = []\n",
    "    subjects = sorted([\n",
    "        d for d in os.listdir(events_root)\n",
    "        if os.path.isdir(os.path.join(events_root, d))\n",
    "    ])\n",
    "\n",
    "    for subject in subjects:\n",
    "        for seq in os.listdir(os.path.join(events_root, subject)):\n",
    "            classif = os.path.join(events_root, subject, seq, \"classif.csv\")\n",
    "            if not os.path.exists(classif):\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(classif, sep=\";\")\n",
    "            for _, r in df.iterrows():\n",
    "                rows.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"seq\": seq,\n",
    "                    \"label\": int(r[\"Class\"]),\n",
    "                    \"t0\": float(r[\"Timestamp Start\"]),\n",
    "                    \"t1\": float(r[\"Timestamp End\"]),\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "segments = build_segments_index(ROOTS[\"events\"])\n",
    "print(\"Total segments:\", len(segments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train segments: 6084\n",
      "Val segments: 1569\n"
     ]
    }
   ],
   "source": [
    "allowed_subjects = {f\"S{str(i).zfill(2)}\" for i in range(1, 25)}\n",
    "segments = segments[segments[\"subject\"].isin(allowed_subjects)].reset_index(drop=True)\n",
    "\n",
    "subjects = sorted(segments[\"subject\"].unique())\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(subjects)\n",
    "\n",
    "n_train = int(0.8 * len(subjects))\n",
    "train_subjects = set(subjects[:n_train])\n",
    "val_subjects   = set(subjects[n_train:])\n",
    "\n",
    "train_segments = segments[segments[\"subject\"].isin(train_subjects)].reset_index(drop=True)\n",
    "val_segments   = segments[segments[\"subject\"].isin(val_subjects)].reset_index(drop=True)\n",
    "\n",
    "print(\"Train segments:\", len(train_segments))\n",
    "print(\"Val segments:\", len(val_segments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMUEventDataset(Dataset):\n",
    "    def __init__(self, segments_df, roots, filenames, L=256):\n",
    "        self.df = segments_df.reset_index(drop=True)\n",
    "        self.roots = roots\n",
    "        self.filenames = filenames\n",
    "        self.L = L\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        subject, seq = r[\"subject\"], r[\"seq\"]\n",
    "        t0, t1 = float(r[\"t0\"]), float(r[\"t1\"])\n",
    "        y = int(r[\"label\"]) - 1  # 0..30\n",
    "\n",
    "        path = os.path.join(\n",
    "            self.roots[\"imu\"], subject, seq, self.filenames[\"imu\"]\n",
    "        )\n",
    "\n",
    "        x = read_and_slice_by_time(path, t0, t1)  # [T,96]\n",
    "        x = resample_to_L(x, self.L)              # [256,96]\n",
    "        x = x.transpose(0, 1).contiguous()        # [96,256]\n",
    "\n",
    "        return {\n",
    "            \"imu\": x,\n",
    "            \"y\": torch.tensor(y, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch: torch.Size([128, 96, 256]) torch.Size([128])\n",
      "Val batch: torch.Size([128, 96, 256]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "train_ds = IMUEventDataset(train_segments, ROOTS, FILENAMES, L)\n",
    "val_ds   = IMUEventDataset(val_segments, ROOTS, FILENAMES, L)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"Train batch:\", b[\"imu\"].shape, b[\"y\"].shape)\n",
    "\n",
    "b = next(iter(val_dl))\n",
    "print(\"Val batch:\", b[\"imu\"].shape, b[\"y\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMU_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=31):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(96, 64, 7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, 5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(dim=-1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 48/48 [03:05<00:00,  3.86s/it, acc=0.414, loss=2.2410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 48/48 [02:52<00:00,  3.59s/it, acc=0.790, loss=0.8031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 48/48 [02:52<00:00,  3.59s/it, acc=0.888, loss=0.4695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 48/48 [02:56<00:00,  3.67s/it, acc=0.923, loss=0.3440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 48/48 [02:53<00:00,  3.61s/it, acc=0.937, loss=0.2949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.805\n"
     ]
    }
   ],
   "source": [
    "def train_imu(model, train_dl, val_dl, epochs=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct = total = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            x = batch[\"imu\"].to(device, non_blocking=True)\n",
    "            y = batch[\"y\"].to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            correct += (model(x).argmax(1) == y).sum().item()\n",
    "            total += x.size(0)\n",
    "\n",
    "            pbar.set_postfix(\n",
    "                loss=f\"{running_loss/total:.4f}\",\n",
    "                acc=f\"{correct/total:.3f}\"\n",
    "            )\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                x = batch[\"imu\"].to(device)\n",
    "                y = batch[\"y\"].to(device)\n",
    "                correct += (model(x).argmax(1) == y).sum().item()\n",
    "                total += x.size(0)\n",
    "\n",
    "        print(f\"Val acc: {correct/total:.3f}\")\n",
    "model = IMU_CNN(num_classes=31).to(device)\n",
    "train_imu(model, train_dl, val_dl, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.805\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x = batch[\"imu\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    return acc\n",
    "    \n",
    "val_acc = evaluate(model, val_dl)\n",
    "print(f\"Validation accuracy: {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vrai label : 10\n",
      "Prédit     : 10\n"
     ]
    }
   ],
   "source": [
    "sample = val_ds[2]\n",
    "\n",
    "x = sample[\"imu\"].unsqueeze(0).to(device)  # [1,96,256]\n",
    "y_true = sample[\"y\"].item()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "    y_pred = logits.argmax(dim=1).item()\n",
    "\n",
    "print(\"Vrai label :\", y_true)\n",
    "print(\"Prédit     :\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fisa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
